{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Workshop\n",
    "# NBA Free Throws Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/free_throw_img.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In January 2015 a data-set of 600K NBA free-throws was upload to Kaggle by Sebastian-Mantey. The data was scraped from the website ESPN.com which belongs to an entertainment and sports programming network. \n",
    "\n",
    "Since the data-set was uploaded, 25 kernels were uploaded to Kaggle related to it. Most of the kernels summarize, analyze and visualize the data and do not try to predict anything. However, there are two interesting kernels:\n",
    "\n",
    "The kernel ‘Shooting percentage over time’, engages with the questions How does the Free Throw shooting percent develop over time? Does it go down as the game approaches the ending due to higher pressure? Does it go up thanks to players being warmer, or alternatively - better shooters take the ball?\n",
    "\n",
    "The findings of the analysis are that in the end of every quarter of a game, there is an increase in the number of free-throws and in the free-throws percentage. They also found that most of the throws in the end of every quarter were performed by better players, but still the absolute time of the throw affected the outcome more than how performed it. \n",
    "\n",
    "Another interesting kernel is ‘Pooling Partial Hierarchica via champs throw Free’, in which they try to find the best players in the data-set with statistical and probability methods such as complete pooling, and find the probability p for every player to succeed in the free-throw. \n",
    "\n",
    "Outside Kaggle, we found the article ‘Mindfulness and Free Throws’. This article investigate the relationship between mindfulness, preshot routine, and basketball free-throw percentage. The findings suggest that the combination of mindfulness levels, skill level (practice free-throw percentage), and competitive experience (year in school), all contribute to the prediction of competitive free throw percentage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from our_models import plot_confusion,logreg_grid_search,random_forest,logreg\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reading the Original Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end_result</th>\n",
       "      <th>game</th>\n",
       "      <th>game_id</th>\n",
       "      <th>period</th>\n",
       "      <th>play</th>\n",
       "      <th>player</th>\n",
       "      <th>playoffs</th>\n",
       "      <th>score</th>\n",
       "      <th>season</th>\n",
       "      <th>shot_made</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrew Bynum makes free throw 1 of 2</td>\n",
       "      <td>Andrew Bynum</td>\n",
       "      <td>regular</td>\n",
       "      <td>0 - 1</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>1</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrew Bynum makes free throw 2 of 2</td>\n",
       "      <td>Andrew Bynum</td>\n",
       "      <td>regular</td>\n",
       "      <td>0 - 2</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>1</td>\n",
       "      <td>11:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrew Bynum makes free throw 1 of 2</td>\n",
       "      <td>Andrew Bynum</td>\n",
       "      <td>regular</td>\n",
       "      <td>18-Dec</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>1</td>\n",
       "      <td>7:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrew Bynum misses free throw 2 of 2</td>\n",
       "      <td>Andrew Bynum</td>\n",
       "      <td>regular</td>\n",
       "      <td>18-Dec</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>0</td>\n",
       "      <td>7:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Shawn Marion makes free throw 1 of 1</td>\n",
       "      <td>Shawn Marion</td>\n",
       "      <td>regular</td>\n",
       "      <td>21-Dec</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>1</td>\n",
       "      <td>7:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Amare Stoudemire makes free throw 1 of 2</td>\n",
       "      <td>Amare Stoudemire</td>\n",
       "      <td>regular</td>\n",
       "      <td>33 - 20</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>1</td>\n",
       "      <td>3:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Amare Stoudemire makes free throw 2 of 2</td>\n",
       "      <td>Amare Stoudemire</td>\n",
       "      <td>regular</td>\n",
       "      <td>34 - 20</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>1</td>\n",
       "      <td>3:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>2</td>\n",
       "      <td>Leandro Barbosa misses free throw 1 of 2</td>\n",
       "      <td>Leandro Barbosa</td>\n",
       "      <td>regular</td>\n",
       "      <td>43 - 29</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>0</td>\n",
       "      <td>10:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>2</td>\n",
       "      <td>Leandro Barbosa makes free throw 2 of 2</td>\n",
       "      <td>Leandro Barbosa</td>\n",
       "      <td>regular</td>\n",
       "      <td>44 - 29</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>1</td>\n",
       "      <td>10:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>2</td>\n",
       "      <td>Lamar Odom makes free throw 1 of 2</td>\n",
       "      <td>Lamar Odom</td>\n",
       "      <td>regular</td>\n",
       "      <td>44 - 30</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>1</td>\n",
       "      <td>10:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  end_result       game    game_id  period  \\\n",
       "0  106 - 114  PHX - LAL  261031013       1   \n",
       "1  106 - 114  PHX - LAL  261031013       1   \n",
       "2  106 - 114  PHX - LAL  261031013       1   \n",
       "3  106 - 114  PHX - LAL  261031013       1   \n",
       "4  106 - 114  PHX - LAL  261031013       1   \n",
       "5  106 - 114  PHX - LAL  261031013       1   \n",
       "6  106 - 114  PHX - LAL  261031013       1   \n",
       "7  106 - 114  PHX - LAL  261031013       2   \n",
       "8  106 - 114  PHX - LAL  261031013       2   \n",
       "9  106 - 114  PHX - LAL  261031013       2   \n",
       "\n",
       "                                       play            player playoffs  \\\n",
       "0      Andrew Bynum makes free throw 1 of 2      Andrew Bynum  regular   \n",
       "1      Andrew Bynum makes free throw 2 of 2      Andrew Bynum  regular   \n",
       "2      Andrew Bynum makes free throw 1 of 2      Andrew Bynum  regular   \n",
       "3     Andrew Bynum misses free throw 2 of 2      Andrew Bynum  regular   \n",
       "4      Shawn Marion makes free throw 1 of 1      Shawn Marion  regular   \n",
       "5  Amare Stoudemire makes free throw 1 of 2  Amare Stoudemire  regular   \n",
       "6  Amare Stoudemire makes free throw 2 of 2  Amare Stoudemire  regular   \n",
       "7  Leandro Barbosa misses free throw 1 of 2   Leandro Barbosa  regular   \n",
       "8   Leandro Barbosa makes free throw 2 of 2   Leandro Barbosa  regular   \n",
       "9        Lamar Odom makes free throw 1 of 2        Lamar Odom  regular   \n",
       "\n",
       "     score       season  shot_made   time  \n",
       "0    0 - 1  2006 - 2007          1  11:45  \n",
       "1    0 - 2  2006 - 2007          1  11:45  \n",
       "2   18-Dec  2006 - 2007          1   7:26  \n",
       "3   18-Dec  2006 - 2007          0   7:26  \n",
       "4   21-Dec  2006 - 2007          1   7:18  \n",
       "5  33 - 20  2006 - 2007          1   3:15  \n",
       "6  34 - 20  2006 - 2007          1   3:15  \n",
       "7  43 - 29  2006 - 2007          0  10:52  \n",
       "8  44 - 29  2006 - 2007          1  10:52  \n",
       "9  44 - 30  2006 - 2007          1  10:37  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_throws_db = pd.read_csv('free_throws.csv')\n",
    "free_throws_db.drop_duplicates()\n",
    "free_throws_db.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describition of dataset:\n",
    "- end_result: host total score - guest total score\n",
    "- game: host team vs guest team\n",
    "- game_id: id of specific game\n",
    "- period: which quarter\n",
    "- play: who make free throw, make or miss free throw\n",
    "- player: player name\n",
    "- playoffs: whether a playoff game or regular game\n",
    "- score: host team score - guest team score at that time\n",
    "- season: NBA season\n",
    "- shot_made: whether player got the free throw\n",
    "- time: time left in that quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of free throws in database: 618019\n",
      "Number of games in database: 12874\n",
      "Games distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "regular     575893\n",
       "playoffs     42126\n",
       "Name: playoffs, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of free throws in database: %d\"%(free_throws_db.shape[0]))\n",
    "print(\"Number of games in database: {}\".format(free_throws_db.game_id.unique().size))\n",
    "print(\"Games distribution:\")\n",
    "free_throws_db['playoffs'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting more data from internet\n",
    "\n",
    "In order to expand our dataset, we decided to use an open source python library PandasBasketball, and use a webscrapper in order to get more players stats from https://www.basketball-reference.com website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of basketball-reference NBA player stats webpage:\n",
    "![title](img/lebron_page_1.jpg)\n",
    "![title](img/lebron_page_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our code to extract from website html our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tools import get_player_stats\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning, module='BeatifulSoup')\n",
    "#dataFrame = get_player_stats(\"Lebron James\")\n",
    "#print(dataFrame.columns)\n",
    "#dataFrame.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns used for this database for each player:\n",
    "- Position : The most common position for the player over his seasons.\n",
    "- FG%\n",
    "- 3P%\n",
    "- FT%\n",
    "- Height\n",
    "- Weight\n",
    "- ShootingHand\n",
    "- draftRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We merged both the datasets according our collected data from internet.\n",
    "Some players stats had been inserted manually because some bugs found on PandasBasketball library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>end_result</th>\n",
       "      <th>game</th>\n",
       "      <th>game_id</th>\n",
       "      <th>period</th>\n",
       "      <th>play</th>\n",
       "      <th>player</th>\n",
       "      <th>playoffs</th>\n",
       "      <th>score</th>\n",
       "      <th>season</th>\n",
       "      <th>...</th>\n",
       "      <th>sec</th>\n",
       "      <th>abs_min</th>\n",
       "      <th>abs_time</th>\n",
       "      <th>scores</th>\n",
       "      <th>scoreDif</th>\n",
       "      <th>First_shot</th>\n",
       "      <th>Second_shot</th>\n",
       "      <th>Third_shot</th>\n",
       "      <th>First_shot_was_in</th>\n",
       "      <th>Second_shot_was_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrew Bynum makes free throw 1 of 2</td>\n",
       "      <td>Andrew Bynum</td>\n",
       "      <td>0</td>\n",
       "      <td>0 - 1</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>['0 ', ' 1']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrew Bynum makes free throw 2 of 2</td>\n",
       "      <td>Andrew Bynum</td>\n",
       "      <td>0</td>\n",
       "      <td>0 - 2</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>['0 ', ' 2']</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>106 - 114</td>\n",
       "      <td>PHX - LAL</td>\n",
       "      <td>261031013</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrew Bynum makes free throw 1 of 2</td>\n",
       "      <td>Andrew Bynum</td>\n",
       "      <td>0</td>\n",
       "      <td>18-12</td>\n",
       "      <td>2006 - 2007</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>274</td>\n",
       "      <td>['18', '12']</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1 end_result       game    game_id  period  \\\n",
       "0             0  106 - 114  PHX - LAL  261031013       1   \n",
       "1             1  106 - 114  PHX - LAL  261031013       1   \n",
       "2             2  106 - 114  PHX - LAL  261031013       1   \n",
       "\n",
       "                                   play        player  playoffs  score  \\\n",
       "0  Andrew Bynum makes free throw 1 of 2  Andrew Bynum         0  0 - 1   \n",
       "1  Andrew Bynum makes free throw 2 of 2  Andrew Bynum         0  0 - 2   \n",
       "2  Andrew Bynum makes free throw 1 of 2  Andrew Bynum         0  18-12   \n",
       "\n",
       "        season  ...  sec abs_min  abs_time        scores  scoreDif  \\\n",
       "0  2006 - 2007  ...   45       1        15  ['0 ', ' 1']         1   \n",
       "1  2006 - 2007  ...   45       1        15  ['0 ', ' 2']         2   \n",
       "2  2006 - 2007  ...   26       5       274  ['18', '12']         6   \n",
       "\n",
       "   First_shot  Second_shot  Third_shot  First_shot_was_in Second_shot_was_in  \n",
       "0           1            0           0                  0                  0  \n",
       "1           0            1           0                  1                  0  \n",
       "2           1            0           0                  0                  0  \n",
       "\n",
       "[3 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_p1 = pd.read_csv(\"database_preprocessed_part1.csv\")\n",
    "database_p2 = pd.read_csv(\"database_preprocessed_part2.csv\")\n",
    "database = pd.concat([database_p1,database_p2])\n",
    "database = database.drop(columns=['Unnamed: 0', 'Unnamed: 0'])\n",
    "# Drop duplicated rows\n",
    "database.drop_duplicates()\n",
    "database.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrePrecessing the data \n",
    "\n",
    "\n",
    "### Specifing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_variables = ['shot_made', 'playoffs', 'ShootingHand']\n",
    "categorical_variables = ['end_result', 'game', 'game_id', 'period', 'play', 'player', 'season','First_shot',\n",
    "                        'Second_shot','Third_shot','First_shot_was_in','Second_shot_was_in']\n",
    "numeric_variables = ['score','time','FG%','2P%', '3P%', 'FT%', 'Height', 'Weight', 'draftRank', 'Pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1          617786\n",
       "end_result            617786\n",
       "game                  617786\n",
       "game_id               617786\n",
       "period                617786\n",
       "play                  617786\n",
       "player                617786\n",
       "playoffs              617786\n",
       "score                 617786\n",
       "season                617786\n",
       "shot_made             617786\n",
       "time                  617786\n",
       "FG%                   617786\n",
       "2P%                   617786\n",
       "3P%                   612884\n",
       "FT%                   570699\n",
       "Height                617786\n",
       "Weight                617786\n",
       "draftRank             617786\n",
       "Pos                   617786\n",
       "ShootingHand          617786\n",
       "Team                  617786\n",
       "Difference            617786\n",
       "minute                617786\n",
       "sec                   617786\n",
       "abs_min               617786\n",
       "abs_time              617786\n",
       "scores                617786\n",
       "scoreDif              617786\n",
       "First_shot            617786\n",
       "Second_shot           617786\n",
       "Third_shot            617786\n",
       "First_shot_was_in     617786\n",
       "Second_shot_was_in    617786\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, we have only two columns with missing data. First - the draftRank column. \n",
    "This values are missing because the players performed the free throw didn't have a draft rank and not because we couldn't collect the data. Second - 3P% has missing values since there are players that have never throws a 3-pointer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing the columns \n",
    "We will change the relevant categorical variable (only the position) to numeric and change the binary and numeric variables to be in the appropriate type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1            int64\n",
       "end_result             object\n",
       "game                   object\n",
       "game_id                 int64\n",
       "period                  int64\n",
       "play                   object\n",
       "player                 object\n",
       "playoffs                int64\n",
       "score                  object\n",
       "season                 object\n",
       "shot_made               int64\n",
       "time                   object\n",
       "FG%                   float64\n",
       "2P%                   float64\n",
       "3P%                   float64\n",
       "FT%                   float64\n",
       "Height                  int64\n",
       "Weight                  int64\n",
       "draftRank               int64\n",
       "Pos                    object\n",
       "ShootingHand            int64\n",
       "Team                   object\n",
       "Difference            float64\n",
       "minute                  int64\n",
       "sec                     int64\n",
       "abs_min                 int64\n",
       "abs_time                int64\n",
       "scores                 object\n",
       "scoreDif                int64\n",
       "First_shot              int64\n",
       "Second_shot             int64\n",
       "Third_shot              int64\n",
       "First_shot_was_in       int64\n",
       "Second_shot_was_in      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding an abs_time column to our database:\n",
    "database['minute'] = database.time.apply(lambda x: int(x[:len(x)-3]))\n",
    "database['sec'] = database.time.apply(lambda x: int(x[len(x)-2:]))\n",
    "database['abs_min'] = 12 - database['minute']+12*(database.period -1)\n",
    "database['abs_time'] = 60*(database.abs_min-1) + 60 - database['sec']\n",
    "\n",
    "# adding a scoreDif column that represents the difference in the groups scores in the time of the throw:\n",
    "database['scores'] = database.score.replace(' - ', '-').apply(lambda x: x.split('-'))\n",
    "database['scoreDif'] = database.scores.apply(lambda x: abs(int(x[1])-int(x[0])))\n",
    "\n",
    "database.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the data \n",
    "\n",
    "\n",
    "### Analyzing the number of throws troughout the game\n",
    "\n",
    "We would like to show the free throws distribution throughout the game time,\n",
    "in our current dataset, the time column represents the time left in that quarter and the period column represents the quarter of the game, so we'll add a new column to our data which calculate the absolute time in the game that the throw was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  counting the num of throws, and success throws precentage per minute\n",
    "minutes = range(int(max(database.abs_min)))\n",
    "total_throws = []\n",
    "success_throws = []\n",
    "success_precentage = []\n",
    "\n",
    "def count_throws(database,minute):    \n",
    "    made = len(database[(database.abs_min == minute) & (database.shot_made == 1)])\n",
    "    success_throws.append(made)\n",
    "    total = len(database[database.abs_min == minute])\n",
    "    total_throws.append(total)\n",
    "    if total == 0:\n",
    "        precentage = 0.0\n",
    "    else:\n",
    "        precentage = made/total\n",
    "    success_precentage.append(precentage)\n",
    "\n",
    "for minute in minutes:\n",
    "    count_throws(database,minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of throws over time\n",
    "plt.plot(minutes,total_throws)\n",
    "plt.title('Number of throws over time')\n",
    "plt.xlim([1,48])\n",
    "plt.ylim([0, 40000])\n",
    "plt.plot([12,12],[0,40000], '--', linewidth = 1, color = 'r')\n",
    "plt.plot([24,24],[0,40000], '--', linewidth = 1, color = 'r')\n",
    "plt.plot([36,36],[0,40000], '--', linewidth = 1, color = 'r')\n",
    "plt.plot([48,48],[0,40000], '--', linewidth = 1, color = 'r')\n",
    "plt.xlabel('Minute')\n",
    "plt.ylabel('num of throws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success throws precentage over time   \n",
    "plt.plot(minutes,success_precentage)\n",
    "plt.title('Scoring % over time - Always worse at the beginning')\n",
    "plt.xlim([1,48])\n",
    "plt.ylim([0.65,0.85])\n",
    "plt.plot([12,12],[0,1], '--', linewidth = 1, color = 'r')\n",
    "plt.plot([24,24],[0,1], '--', linewidth = 1, color = 'r')\n",
    "plt.plot([36,36],[0,1], '--', linewidth = 1, color = 'r')\n",
    "plt.plot([48,48],[0,1], '--', linewidth = 1, color = 'r')\n",
    "plt.xlabel('Minute')\n",
    "plt.ylabel('Free Throws %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots we can observe that at the begining of every querter, both the number of free throws and the success precentage drops.\n",
    "Moreover, at the end of a quarter, and especially at the end of the game, both plots increase.\n",
    "We can explain this behaivor, as basketball rules when a team made more than 5 fouls, every another foul made in that quarter will be penalty with a free throw.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the number of throws troughout the game\n",
    "We want to see how the average of free throws attempted and succeed in games over our differents season in our dataset are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_attempted_per_game = database.groupby([\"season\", \"playoffs\"])['shot_made'].count().unstack()\n",
    "shot_made_per_game = database.groupby([\"season\", \"playoffs\"])['shot_made'].sum().unstack()\n",
    "\n",
    "# this has to be divided by the number of games for each season to get an average\n",
    "number_of_games=database.groupby([\"season\", \"playoffs\"])['game_id'].nunique().unstack()\n",
    "\n",
    "average_shot_made_per_game = shot_made_per_game/number_of_games\n",
    "average_shot_attempted_per_game = shot_attempted_per_game/number_of_games\n",
    "\n",
    "\n",
    "f, (ax1) = plt.subplots(figsize=(18,18))\n",
    "first=average_shot_attempted_per_game.plot(ax=ax1, marker='o', figsize=(15,8), xticks=range(10), color=['b','r'], rot=90)\n",
    "second=average_shot_made_per_game.plot(ax=ax1, marker='o', linestyle='--', figsize=(15,8), xticks=range(10), color=['b','r'], rot=90)\n",
    "ax1.set_title('Average number of free throws per period. Attempted vs Successfula', size=25)\n",
    "legend=plt.legend((' playoffs attempted','regular attempted','playoffs successful','regular successful'), loc=6)\n",
    "ax1.add_artist(legend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that are not very big differences over the seasons at both attempted and successful shots.\n",
    "But there is a clear difference between the amount of made shots and and successful shots at playoffs and regular season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_shot_perc , missed_shot_perc = database['shot_made'].value_counts(normalize=True) * 100\n",
    "print(\"Made shots percentage in dataset: %.2f\"%(made_shot_perc))\n",
    "print(\"Missed shots percentage in dataset: %.2f\"%(missed_shot_perc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correlation between the player's draft rank and the free-throw result\n",
    "\n",
    "We wanted to check if there is a conection between the draft rank of the player performing the shot and the result of the shot. \n",
    "We have some missing values in the draftRank column, and it was because the player throwing had no draft rank, so we will fill the NAN cells with 61."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function that replace the field \"undrafted\" with a 0 in draftRank column\n",
    "\n",
    "database['draftRank'] = database['draftRank'].replace(np.nan, 61)\n",
    "database['draftRank'] = database['draftRank'].replace(\"undrafted\", 61)\n",
    "# pd.to_numeric(free_throws_db['draftRank'])\n",
    "database['draftRank'] = database.draftRank.apply(lambda x: int(float(x)))\n",
    "\n",
    "np.nanmax(database['draftRank'])\n",
    "\n",
    "ranks = range(1, int(np.nanmax(database['draftRank']))+1)\n",
    "success_precentage_by_rank = []\n",
    "\n",
    "def throws_per_rank(database,rank):    \n",
    "    total = len(database[database.draftRank == rank])\n",
    "    if total == 0:\n",
    "        precentage = 0.0\n",
    "    else:\n",
    "        made = len(database[(database.draftRank == rank) & (database.shot_made == 1)])\n",
    "        precentage = made/total\n",
    "    success_precentage_by_rank.append(precentage)\n",
    "\n",
    "for rank in ranks:\n",
    "    throws_per_rank(database,rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Success throws precentage over time\n",
    "plt.plot(list(ranks),success_precentage_by_rank)\n",
    "plt.title('Success Throws percentage as function of Draft Rank  ')\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Draft Rank')\n",
    "plt.ylabel('Free Throws %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there is no special trend in the graph (the FT% is between 0.7 to 0.9 for all ranks). We expected that the higher-ranked players would have better performence but we can't conclude it from the data. \n",
    "\n",
    "We have to take under consideration that the draft-rank data is not uniformly distributed, as we can see in the graph below, and is biased towared small values (the value 0 represents missing values, and in our case players who had no draft rank), so the barplot above needs to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(database['draftRank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correlation between the score difference and the free-throw result\n",
    "\n",
    "We will add a column calculating the differance.\n",
    "We assume that if the difference is small in the time of the shot, the player would be more stressed and his shooting precentage would drop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difs = range(int(np.min(database['scoreDif']))+1,int(np.max(database['scoreDif']))+1)\n",
    "success_precentage_by_scoreDif = []\n",
    "\n",
    "def throws_per_dif(database,dif):    \n",
    "    total = len(database[database.scoreDif == dif])\n",
    "    if total == 0:\n",
    "        precentage = 0.0\n",
    "    else:\n",
    "        made = len(database[(database.scoreDif == dif) & (database.shot_made == 1)])\n",
    "        precentage = made/total\n",
    "    success_precentage_by_scoreDif.append(precentage)\n",
    "\n",
    "for dif in difs:\n",
    "    throws_per_dif(database,dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success throws precentage over time\n",
    "plt.bar(list(difs),success_precentage_by_scoreDif)\n",
    "plt.title('Success Throws percentage as function of Score Difference')\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Score Difference')\n",
    "plt.ylabel('Free Throws %')\n",
    "plt.show()\n",
    "plt.bar(list(difs),success_precentage_by_scoreDif)\n",
    "plt.title('Success Throws percentage as function of Score Difference')\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Score Difference')\n",
    "plt.ylabel('Free Throws %')\n",
    "plt.xlim(0,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the 'Success Throws percentage as function of Score Difference' plot, it seems that our hypotesis that as long as the score difference gets bigger, so as the free-throw success precentaege is partly true. \n",
    "We can see a trend in the graph but it is not continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correlation between the player position and the free-throw result\n",
    "We want to analyze if the \"Position\" feature of each player could be a helpful feature that may help our prediction model.\n",
    "Above we can see a plot showing the distribution of the FT% over the different positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = sns.catplot(x=\"Pos\", y=\"FT%\", data=database);\n",
    "#ax2 = sns.catplot(x=\"Pos\", y=\"FT%\",  kind=\"swarm\", data=database);\n",
    "ax2.set(xlabel='Player Position', ylabel='Throws Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correlation between all variables\n",
    "from preProcess import find_correlations\n",
    "find_correlations(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average FT% at each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.groupby('Pos').agg({'FT%':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Centers players owns the worst percentage.\n",
    "\n",
    "Point Guards and Shouting Guards players holds the best percentage over all the positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will check how well our dataset represents the general population of NBA Players.\n",
    "1. MSE of the FT% players achieved in their carrers and their FT% we have in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our goal\n",
    "We’ll try to explore two options for prediction:\n",
    "\n",
    "•\tOffline prediction: try to predict the players succes percentage (for all kinds of throws) for the current season based on his past performance and give the coach an insight about the player. \n",
    "\n",
    "•\tOnline prediction : try to predict whether a player will hit in a certain throw. \n",
    "\n",
    "### What's next?\n",
    "\n",
    "- Extract more statistical analysis on our current data, find corralation between more than two parameters. \n",
    "- Pre-process our data in order to create an initial ML model.\n",
    "- Achieve good results on our ML model\n",
    "- Try to reduce our problem dimension/features by analyzing which features have big influence.\n",
    "- \"Engineer\" more strong features in order to achieve dimension reduction.\n",
    "- Build new ML model and achieve good results, again! YEY!\n",
    "- Explain our ML model results, when it fails and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Fitting a model\n",
    " \n",
    " We will choose the relevant columns (ommiting most of the categorical variables) from our model_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the position variable from categorial variable to numeric variable:\n",
    "from preProcess import preProcess_position\n",
    "database = preProcess_position(database)\n",
    "\n",
    "# changing the playoff and ShootingHand to binary variables:\n",
    "database['playoffs'] = database['playoffs'].map(lambda x : 0 if x == 'regular' else 1)\n",
    "database['ShootingHand'] = database['ShootingHand'].map(lambda x : 0 if x == 'Right' else 1)\n",
    "\n",
    "# modify the numeric variables Height and Weight:\n",
    "database['Height'] = database['Height'].map(lambda x : int(x))\n",
    "database['Weight'] = database['Weight'].map(lambda x : int(x))\n",
    "\n",
    "# as we saw earlier, we have missing data in the 'FT%' and the '3P%' columns so we will fill the NA cells with the mode of the column\n",
    "database['FT%'] = database['FT%'].fillna(database['FT%'].mode()[0])\n",
    "database['3P%'] = database['3P%'].fillna(database['FT%'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done pre-processing the data for now.\n",
    "### Starting to prepare data for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567263, 34)\n",
      "Shots made are 0.76 % of dataset\n",
      "Shots missed are 0.24 % of dataset\n"
     ]
    }
   ],
   "source": [
    "database_p1 = pd.read_csv(\"database_preprocessed_part1.csv\")\n",
    "database_p2 = pd.read_csv(\"database_preprocessed_part2.csv\")\n",
    "database = pd.concat([database_p1,database_p2])\n",
    "database = database.drop(columns=['Unnamed: 0', 'Unnamed: 0'])\n",
    "database = database.dropna()\n",
    "\n",
    "print(database.shape)\n",
    "counts = database.shot_made.value_counts()\n",
    "print(f\"Shots made are {counts[1]/database.shape[0]:.2f} % of dataset\")\n",
    "print(f\"Shots missed are {counts[0]/database.shape[0]:.2f} % of dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing our model's parameters from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FG%</th>\n",
       "      <th>2P%</th>\n",
       "      <th>3P%</th>\n",
       "      <th>FT%</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>draftRank</th>\n",
       "      <th>abs_min</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>567263.000000</td>\n",
       "      <td>567263.000000</td>\n",
       "      <td>567263.000000</td>\n",
       "      <td>567263.000000</td>\n",
       "      <td>567263.000000</td>\n",
       "      <td>567263.000000</td>\n",
       "      <td>567263.000000</td>\n",
       "      <td>567263.000000</td>\n",
       "      <td>567263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.464319</td>\n",
       "      <td>0.489615</td>\n",
       "      <td>0.294291</td>\n",
       "      <td>0.749806</td>\n",
       "      <td>200.642212</td>\n",
       "      <td>101.117845</td>\n",
       "      <td>19.469024</td>\n",
       "      <td>27.732487</td>\n",
       "      <td>0.625861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.046051</td>\n",
       "      <td>0.038692</td>\n",
       "      <td>0.113909</td>\n",
       "      <td>0.091865</td>\n",
       "      <td>9.112468</td>\n",
       "      <td>12.559274</td>\n",
       "      <td>17.656197</td>\n",
       "      <td>14.120636</td>\n",
       "      <td>10.493966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.708520</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.768688</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.493000</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.808055</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 FG%            2P%            3P%            FT%  \\\n",
       "count  567263.000000  567263.000000  567263.000000  567263.000000   \n",
       "mean        0.464319       0.489615       0.294291       0.749806   \n",
       "std         0.046051       0.038692       0.113909       0.091865   \n",
       "min         0.136000       0.188000       0.000000       0.000000   \n",
       "25%         0.434000       0.466000       0.275000       0.708520   \n",
       "50%         0.453000       0.482000       0.335000       0.768688   \n",
       "75%         0.493000       0.509000       0.366000       0.808055   \n",
       "max         0.669000       0.681000       1.000000       1.000000   \n",
       "\n",
       "              Height         Weight      draftRank        abs_min  \\\n",
       "count  567263.000000  567263.000000  567263.000000  567263.000000   \n",
       "mean      200.642212     101.117845      19.469024      27.732487   \n",
       "std         9.112468      12.559274      17.656197      14.120636   \n",
       "min       165.000000      61.000000       1.000000       1.000000   \n",
       "25%       193.000000      92.000000       5.000000      16.000000   \n",
       "50%       201.000000     100.000000      14.000000      28.000000   \n",
       "75%       208.000000     111.000000      28.000000      40.000000   \n",
       "max       229.000000     147.000000      61.000000      96.000000   \n",
       "\n",
       "          Difference  \n",
       "count  567263.000000  \n",
       "mean        0.625861  \n",
       "std        10.493966  \n",
       "min       -63.000000  \n",
       "25%        -6.000000  \n",
       "50%         1.000000  \n",
       "75%         7.000000  \n",
       "max        59.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we choose the relevant variables for our model:\n",
    "continuous_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'draftRank','abs_min', 'Difference']\n",
    "binary_parameters = ['playoffs', 'Pos', 'ShootingHand','First_shot',\n",
    "                        'Second_shot','Third_shot','First_shot_was_in','Second_shot_was_in'] \n",
    "binary_database = pd.DataFrame(database[binary_parameters])\n",
    "database[continuous_parameters].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "For our model, we assume that our variables are normally distributed.\n",
    "We will examine this hypothesis and normalize the variables\n",
    "\n",
    "We'll plot the distribution plot of the variables 'FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference'\n",
    "(in 3 different plots because of the scale diffrence) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(database['FG%'], color = 'red', label = \"FG%\") \n",
    "sns.distplot(database['2P%'], color = 'green', label = \"2P%\") \n",
    "sns.distplot(database['3P%'], color = 'orange', label = \"3P%\") \n",
    "sns.distplot(database['FT%'], color = 'blue', label = \"FT%\") \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(database['Weight'], color = 'orange', label = \"Weight [kgs]\") \n",
    "sns.distplot(database['Height'], color = 'blue', label = \"Height [cms]\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(database['Difference'], color = 'blue') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(database['abs_min'], color = 'blue', label = 'abs_min distribution') \n",
    "sns.distplot(database['draftRank'], color = 'red', label = 'draftRank distribution') \n",
    "plt.legend(labels=['abs_time distribution','draftRank distribution'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset to test-train to avoid data-leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shots made are 0.76 % of train dataset\n",
      "Shots missed are 0.24 % of train dataset\n",
      "--------------------------------------\n",
      "Shots made are 0.76 % of test dataset\n",
      "Shots missed are 0.24 % of test dataset\n"
     ]
    }
   ],
   "source": [
    "## SPLITTING DATASET TO TRAIN AND TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "v = database.player.value_counts()\n",
    "database = database[database.player.isin(v.index[v.gt(200)])] ## leave only players with more than 200 games.\n",
    "\n",
    "model_parameters = ['player','playoffs', 'Pos', 'ShootingHand', 'FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight',\n",
    "                    'draftRank','abs_min', 'Difference','shot_made','First_shot',\n",
    "                    'Second_shot','Third_shot','First_shot_was_in','Second_shot_was_in']\n",
    "model_database = database[model_parameters]\n",
    "model_database.describe()\n",
    "\n",
    "# Keep small database with original FT%\n",
    "db_for_baseline_model = model_database[[\"FT%\",\"shot_made\"]]\n",
    "\n",
    "\n",
    "Y = model_database['shot_made']\n",
    "X = model_database.drop(columns=['shot_made','player'])\n",
    "\n",
    "X_raw_train, X_raw_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "train_counts = y_train.value_counts()\n",
    "print(f\"Shots made are {train_counts[1]/y_train.shape[0]:.2f} % of train dataset\")\n",
    "print(f\"Shots missed are {train_counts[0]/y_train.shape[0]:.2f} % of train dataset\")\n",
    "print(\"--------------------------------------\")\n",
    "test_counts = y_test.value_counts()\n",
    "print(f\"Shots made are {test_counts[1]/y_test.shape[0]:.2f} % of test dataset\")\n",
    "print(f\"Shots missed are {test_counts[0]/y_test.shape[0]:.2f} % of test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most of the variables are distribute with a gaussian shape.\n",
    "So at different models we will try different methods for normalization or standartization of the features.\n",
    "\n",
    "Example of standartization and normalization of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "example = X_raw_train.copy()\n",
    "example[semiNormal_parameters] = sc.fit_transform(example[semiNormal_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(example['FG%'], color = 'red') \n",
    "sns.distplot(example['2P%'], color = 'green') \n",
    "sns.distplot(example['FT%'], color = 'blue') \n",
    "sns.distplot(example['3P%'], color = 'orange') \n",
    "plt.legend(labels=['FG% distribution', '2P% distribution', 'FT% distribution', '3P% distribution'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(example['Weight'], color = 'orange') \n",
    "sns.distplot(example['Height'], color = 'blue') \n",
    "plt.legend(labels=['Weight distribution', 'Height distribution'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(example['Difference'], color = 'blue') \n",
    "plt.legend(labels=['Difference distribution'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of using min-max normalization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "nonNormal_parameters = ['abs_min', 'draftRank']\n",
    "example[nonNormal_parameters] = mm.fit_transform(example[nonNormal_parameters])\n",
    "sns.distplot(example['abs_min'], color = 'blue', label = 'abs_min distribution') \n",
    "sns.distplot(example['draftRank'], color = 'red', label = 'draftRank distribution') \n",
    "plt.legend(labels=['abs_time distribution','draftRank distribution'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "Predicit free-throw success only with FT% feature. No learning process or algorithm is used here.\n",
    "Threshold classificator, FT% over 0.5 will be predicted as 1, else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline model\n",
    "from sklearn import metrics\n",
    "\n",
    "baseline_predict = db_for_baseline_model[\"FT%\"].apply(lambda x: 1 if x>=0.6 else 0)\n",
    "expected = database.shot_made\n",
    "print(metrics.classification_report(expected, baseline_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic Regression\n",
    "- Positions are transformed to numerical\n",
    "- semiNormal features standarted to mean=0, std=1\n",
    "- nonNormal_parameters rescaled to 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     42604\n",
      "           1       0.76      1.00      0.86    136296\n",
      "\n",
      "    accuracy                           0.76    178900\n",
      "   macro avg       0.38      0.50      0.43    178900\n",
      "weighted avg       0.58      0.76      0.66    178900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "# Pre-process for this model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mm = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "nonNormal_parameters = ['abs_min', 'draftRank','First_shot',\n",
    "                    'Second_shot','Third_shot','First_shot_was_in','Second_shot_was_in']\n",
    "\n",
    "X_train = X_raw_train.copy()\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "X_train[semiNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters])\n",
    "X_test[semiNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters])\n",
    "X_train[nonNormal_parameters] = mm.fit_transform(X_train[nonNormal_parameters])\n",
    "X_test[nonNormal_parameters] = mm.fit_transform(X_test[nonNormal_parameters])\n",
    "\n",
    "# changing the position variable from categorial variable to numeric variable:\n",
    "from preProcess import preProcess_position\n",
    "X_train = preProcess_position(X_train)\n",
    "X_test = preProcess_position(X_test)\n",
    "## End pre-process\n",
    "\n",
    "print(logreg(X_train.values, y_train.values,X_test.values, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Model\n",
    "- Positions are transformed to numerical\n",
    "- semiNormal features standarted to mean=0, std=1\n",
    "- nonNormal_parameters rescaled to 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import linear_model\n",
    "# from sklearn import metrics\n",
    "\n",
    "# # Pre-process for this model\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# mm = MinMaxScaler()\n",
    "# sc = StandardScaler()\n",
    "# semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "# nonNormal_parameters = ['abs_min', 'draftRank']\n",
    "\n",
    "# X_train = X_raw_train.copy()\n",
    "# X_test = X_raw_test.copy()\n",
    "\n",
    "# X_train[semiNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters])\n",
    "# X_test[semiNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters])\n",
    "# X_train[nonNormal_parameters] = mm.fit_transform(X_train[nonNormal_parameters])\n",
    "# X_test[nonNormal_parameters] = mm.fit_transform(X_test[nonNormal_parameters])\n",
    "\n",
    "# # changing the position variable from categorial variable to numeric variable:\n",
    "# from preProcess import preProcess_position\n",
    "# X_train = preProcess_position(X_train)\n",
    "# X_test = preProcess_position(X_test)\n",
    "# ## End pre-process\n",
    "\n",
    "\n",
    "# # Linear SVM Model\n",
    "# from sklearn.svm import SVC\n",
    "# linear_svm_model = SVC(kernel='linear', class_weight='balanced',verbose=True,C=1)\n",
    "# linear_svm_model.fit(X_train.values, y_train.values)\n",
    "# print(\"Training DONE!\")\n",
    "# predictions = linear_svm_model.predict(X_test.values)\n",
    "# print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net - binary_crossentropy LOSS\n",
    "- Position feature one-hod encoded\n",
    "- All features standarted to mean=0, std=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preProcess import detect_and_remove_outliers\n",
    "\n",
    "# Pre-process for this model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mm = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "nonNormal_parameters = ['abs_min', 'draftRank']\n",
    "\n",
    "X_train = X_raw_train.copy()\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "X_train[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters+nonNormal_parameters])\n",
    "X_test[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters+nonNormal_parameters])\n",
    "\n",
    "onehot_pos_col = pd.get_dummies(X_train['Pos'],prefix='Pos')\n",
    "X_train = pd.concat([X_train,onehot_pos_col],axis=1)\n",
    "X_train = X_train.drop(columns=['Pos'])\n",
    "print(X_train.columns)\n",
    "onehot_pos_col = pd.get_dummies(X_test['Pos'],prefix='Pos')\n",
    "X_test = pd.concat([X_test,onehot_pos_col],axis=1)\n",
    "X_test = X_test.drop(columns=['Pos'])\n",
    "## End pre-process\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adagrad\n",
    "from sklearn import metrics\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=X_train.shape[1]))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(20))#, activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))#, activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))#, activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adagrad(learning_rate=0.001), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=256)\n",
    "print(\"Metrics summary for training\")\n",
    "print(metrics.classification_report(y_train, np.round(model.predict(X_train))))\n",
    "\n",
    "print(\"Metrics summary for test\")\n",
    "print(metrics.classification_report(y_test, np.round(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that because our dataset is not balanced, our models tend to predict '1' (shot made) most of the time, we have a problem. \n",
    "#### If we look at the recall parameter on label '0' (shot missed) we can see that we receive really bad perfomance on this metric.\n",
    "#### We will try to attack this problem with some different methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OverSampling try\n",
    "\n",
    "One common way to tackle the issue of imbalanced data is over-sampling. Over-sampling refers to various methods that aim to increase the number of instances from the underrepresented class in the data set. In our case, these techniques will increase the number of fraudulent transactions in our data to 50:50 (instead of 25:75).\n",
    "\n",
    "The easiest way to do so is to randomly select observations from the minority class and add them to the data set until we achieve a balance between the majority and minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from preProcess import detect_and_remove_outliers\n",
    "\n",
    "# Pre-process for this model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mm = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "nonNormal_parameters = ['abs_min', 'draftRank']\n",
    "\n",
    "X_train = X_raw_train.copy()\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "X_train[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters+nonNormal_parameters])\n",
    "X_test[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters+nonNormal_parameters])\n",
    "\n",
    "onehot_pos_col = pd.get_dummies(X_train['Pos'],prefix='Pos')\n",
    "X_train = pd.concat([X_train,onehot_pos_col],axis=1)\n",
    "X_train = X_train.drop(columns=['Pos'])\n",
    "print(X_train.columns)\n",
    "onehot_pos_col = pd.get_dummies(X_test['Pos'],prefix='Pos')\n",
    "X_test = pd.concat([X_test,onehot_pos_col],axis=1)\n",
    "X_test = X_test.drop(columns=['Pos'])\n",
    "## End pre-process\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adagrad\n",
    "from sklearn import metrics\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=X_train.shape[1]))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(20))#, activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))#, activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))#, activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adagrad(learning_rate=0.001), metrics=['accuracy'])\n",
    "history = model.fit(X_train_resampled, y_train_resampled, epochs=100, batch_size=256)\n",
    "print(\"Metrics summary for training\")\n",
    "print(metrics.classification_report(y_train, np.round(model.predict(X_train))))\n",
    "\n",
    "print(\"Metrics summary for test\")\n",
    "print(metrics.classification_report(y_test, np.round(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that this method helps us to achieve better recall metric on label '0', but we receive a -10% perfomance on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FocalLOSS try\n",
    "\n",
    "focal loss down-weights the well-classified examples. This has the net effect of putting more training emphasis on that data that is hard to classify. In a practical setting where we have a data imbalance, our majority class will quickly become well-classified since we have much more data for it. Thus, in order to insure that we also achieve high accuracy on our minority class, we can use the focal loss to give those minority class examples more relative weight during training.\n",
    "Paper: https://arxiv.org/abs/1708.02002\n",
    "\n",
    "![title](img/focal_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preProcess import detect_and_remove_outliers\n",
    "\n",
    "# Pre-process for this model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mm = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "nonNormal_parameters = ['abs_min', 'draftRank']\n",
    "\n",
    "X_train = X_raw_train.copy()\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "X_train[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters+nonNormal_parameters])\n",
    "X_test[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters+nonNormal_parameters])\n",
    "\n",
    "onehot_pos_col = pd.get_dummies(X_train['Pos'],prefix='Pos')\n",
    "X_train = pd.concat([X_train,onehot_pos_col],axis=1)\n",
    "X_train = X_train.drop(columns=['Pos'])\n",
    "onehot_pos_col = pd.get_dummies(X_test['Pos'],prefix='Pos')\n",
    "X_test = pd.concat([X_test,onehot_pos_col],axis=1)\n",
    "X_test = X_test.drop(columns=['Pos'])\n",
    "\n",
    "X_train, y_model_train = detect_and_remove_outliers(X_train, y_train)\n",
    "\n",
    "## End pre-process\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "# Define our custom loss function\n",
    "def focal_loss(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adagrad\n",
    "from sklearn import metrics\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=X_train.shape[1]))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(20))#, activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))#, activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))#, activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=[focal_loss], optimizer=Adagrad(learning_rate=0.001), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_model_train, epochs=200, batch_size=256)\n",
    "print(\"Metrics summary for training\")\n",
    "print(metrics.classification_report(y_model_train, np.round(model.predict(X_train))))\n",
    "\n",
    "print(\"Metrics summary for test\")\n",
    "print(metrics.classification_report(y_test, np.round(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also in this case, we get an improvement on our label '0' recall , but our accuracy performance is still lower comparing to the baseline model\n",
    "#### Comparing against the model trained with overSampling we receive a lower perfomance on '0' recall metric, but a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preProcess import run_PCA, find_correlations,detect_and_remove_outliers\n",
    "# from models import logreg\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import linear_model\n",
    "# from sklearn import metrics\n",
    "\n",
    "# v = model_database.player.value_counts()\n",
    "# log_reg_database = model_database[model_database.player.isin(v.index[v.gt(200)])] ## leave only players with more than 200 games.\n",
    "# Y = log_reg_database['shot_made']\n",
    "# X = log_reg_database.drop(columns=['shot_made','player'])\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "# number_of_components = 2\n",
    "# X_train, X_test = run_PCA(X_train, X_test,number_of_components)\n",
    "\n",
    "# print(logreg(X_train, Y_train,X_test, Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost regressor model with Shap visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import shap\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from preProcess import detect_and_remove_outliers\n",
    "\n",
    "# Pre-process for this model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mm = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "nonNormal_parameters = ['abs_min', 'draftRank','First_shot',\n",
    "                    'Second_shot','Third_shot','First_shot_was_in','Second_shot_was_in']\n",
    "\n",
    "X_train = X_raw_train.copy()\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "X_train[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters+nonNormal_parameters])\n",
    "X_test[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters+nonNormal_parameters])\n",
    "\n",
    "onehot_pos_col = pd.get_dummies(X_train['Pos'],prefix='Pos')\n",
    "X_train = pd.concat([X_train,onehot_pos_col],axis=1)\n",
    "X_train = X_train.drop(columns=['Pos'])\n",
    "onehot_pos_col = pd.get_dummies(X_test['Pos'],prefix='Pos')\n",
    "X_test = pd.concat([X_test,onehot_pos_col],axis=1)\n",
    "X_test = X_test.drop(columns=['Pos'])\n",
    "\n",
    "X_train, y_model_train = detect_and_remove_outliers(X_train, y_train)\n",
    "\n",
    "## End pre-process\n",
    "\n",
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "model = xgboost.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                             max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "model.fit(X_train,y_model_train)\n",
    "\n",
    "\n",
    "print(\"Metrics summary for training\")\n",
    "print(metrics.classification_report(y_model_train, np.round(model.predict(X_train))))\n",
    "\n",
    "print(\"Metrics summary for test\")\n",
    "print(metrics.classification_report(y_test, np.round(model.predict(X_test))))\n",
    "\n",
    "\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So we understand now, our imbalanced dataset is a problem, and our tries to fix this unbalance results as lower accuracy performance.\n",
    "##### We will try to train different models to understand which features are the dominant in the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost classifier model with Shap visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import shap\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Pre-process for this model\n",
    "from preProcess import detect_and_remove_outliers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mm = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "nonNormal_parameters = ['abs_min', 'draftRank','First_shot',\n",
    "                    'Second_shot','Third_shot','First_shot_was_in','Second_shot_was_in']\n",
    "\n",
    "X_train = X_raw_train.copy()\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "X_train[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters+nonNormal_parameters])\n",
    "X_test[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters+nonNormal_parameters])\n",
    "\n",
    "onehot_pos_col = pd.get_dummies(X_train['Pos'],prefix='Pos')\n",
    "X_train = pd.concat([X_train,onehot_pos_col],axis=1)\n",
    "X_train = X_train.drop(columns=['Pos'])\n",
    "onehot_pos_col = pd.get_dummies(X_test['Pos'],prefix='Pos')\n",
    "X_test = pd.concat([X_test,onehot_pos_col],axis=1)\n",
    "X_test = X_test.drop(columns=['Pos'])\n",
    "\n",
    "X_train, y_model_train = detect_and_remove_outliers(X_train, y_train)\n",
    "\n",
    "## End pre-process\n",
    "\n",
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "model = XGBClassifier()\n",
    " \n",
    "# fit the model with the training data\n",
    "model.fit(X_train,y_model_train)\n",
    "\n",
    " \n",
    "print(\"Metrics summary for training\")\n",
    "print(metrics.classification_report(y_model_train, np.round(model.predict(X_train))))\n",
    "\n",
    "print(\"Metrics summary for test\")\n",
    "print(metrics.classification_report(y_test, np.round(model.predict(X_test))))\n",
    "\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost classifier model with FocalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import shap\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Pre-process for this model\n",
    "from preProcess import detect_and_remove_outliers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mm = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "nonNormal_parameters = ['abs_min', 'draftRank']\n",
    "\n",
    "X_train = X_raw_train.copy()\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "X_train[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters+nonNormal_parameters])\n",
    "X_test[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters+nonNormal_parameters])\n",
    "\n",
    "onehot_pos_col = pd.get_dummies(X_train['Pos'],prefix='Pos')\n",
    "X_train = pd.concat([X_train,onehot_pos_col],axis=1)\n",
    "X_train = X_train.drop(columns=['Pos'])\n",
    "onehot_pos_col = pd.get_dummies(X_test['Pos'],prefix='Pos')\n",
    "X_test = pd.concat([X_test,onehot_pos_col],axis=1)\n",
    "X_test = X_test.drop(columns=['Pos'])\n",
    "\n",
    "X_train, y_model_train = detect_and_remove_outliers(X_train, y_train)\n",
    "\n",
    "## End pre-process\n",
    "\n",
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "\n",
    "xgboster_focal = imb_xgb(special_objective='focal')\n",
    "CV_focal_booster = GridSearchCV(xgboster_focal, {\"focal_gamma\":[2.0]})\n",
    "CV_focal_booster.fit(X_train.values, y_model_train.values)\n",
    "opt_focal_booster = CV_focal_booster.best_estimator_\n",
    "\n",
    "print(\"Metrics summary for training\")\n",
    "print(metrics.classification_report(y_model_train,opt_focal_booster.predict_determine(X_train.values)))\n",
    "\n",
    "print(\"Metrics summary for test\")\n",
    "print(metrics.classification_report(y_test, opt_focal_booster.predict_determine(X_test.values)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.24      0.28     42604\n",
      "           1       0.78      0.85      0.81    136296\n",
      "\n",
      "    accuracy                           0.70    178900\n",
      "   macro avg       0.56      0.55      0.55    178900\n",
      "weighted avg       0.68      0.70      0.69    178900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Pre-process for this model\n",
    "from preProcess import detect_and_remove_outliers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mm = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "nonNormal_parameters = ['abs_min', 'draftRank','First_shot',\n",
    "                    'Second_shot','Third_shot','First_shot_was_in','Second_shot_was_in']\n",
    "\n",
    "X_train = X_raw_train.copy()\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "X_train[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters+nonNormal_parameters])\n",
    "X_test[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters+nonNormal_parameters])\n",
    "\n",
    "onehot_pos_col = pd.get_dummies(X_train['Pos'],prefix='Pos')\n",
    "X_train = pd.concat([X_train,onehot_pos_col],axis=1)\n",
    "X_train = X_train.drop(columns=['Pos'])\n",
    "onehot_pos_col = pd.get_dummies(X_test['Pos'],prefix='Pos')\n",
    "X_test = pd.concat([X_test,onehot_pos_col],axis=1)\n",
    "X_test = X_test.drop(columns=['Pos'])\n",
    "\n",
    "X_train, y_model_train = detect_and_remove_outliers(X_train, y_train)\n",
    "\n",
    "## End pre-process\n",
    "\n",
    "print(random_forest(X_train.values, y_model_train.values,X_test.values, y_test,threshold_flag=False))\n",
    "print(random_forest(X_train.values, y_model_train.values,X_test.values, y_test,threshold_flag=True))\n",
    "\n",
    "#shap.initjs()\n",
    "#explainer = shap.TreeExplainer(rf)\n",
    "#shap_values = explainer.shap_values(X_train)\n",
    "#shap.summary_plot(shap_values, X_train)\n",
    "#shap.summary_plot(shap_values, X_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearset Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process for this model\n",
    "from preProcess import detect_and_remove_outliers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mm = MinMaxScaler()\n",
    "sc = StandardScaler()\n",
    "semiNormal_parameters = ['FG%', '2P%', '3P%', 'FT%', 'Height', 'Weight', 'Difference']\n",
    "nonNormal_parameters = ['abs_min', 'draftRank']\n",
    "\n",
    "X_train = X_raw_train.copy()\n",
    "X_test = X_raw_test.copy()\n",
    "\n",
    "X_train[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_train[semiNormal_parameters+nonNormal_parameters])\n",
    "X_test[semiNormal_parameters+nonNormal_parameters] = sc.fit_transform(X_test[semiNormal_parameters+nonNormal_parameters])\n",
    "\n",
    "onehot_pos_col = pd.get_dummies(X_train['Pos'],prefix='Pos')\n",
    "X_train = pd.concat([X_train,onehot_pos_col],axis=1)\n",
    "X_train = X_train.drop(columns=['Pos'])\n",
    "onehot_pos_col = pd.get_dummies(X_test['Pos'],prefix='Pos')\n",
    "X_test = pd.concat([X_test,onehot_pos_col],axis=1)\n",
    "X_test = X_test.drop(columns=['Pos'])\n",
    "\n",
    "X_train, y_model_train = detect_and_remove_outliers(X_train, y_train)\n",
    "\n",
    "## End pre-process\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski',p=2)\n",
    "knn.fit(X_train, y_model_train)\n",
    "\n",
    "print(\"Metrics summary for training\")\n",
    "print(metrics.classification_report(y_model_train, knn.predict(X_train)))\n",
    "\n",
    "print(\"Metrics summary for test\")\n",
    "print(metrics.classification_report(y_test, knn.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting pairwise distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.plotting import scatter_matrix\n",
    "# scatter_matrix(xgboost_dataset.iloc[:,0:10], figsize=(15,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
